{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM01JWZAKTRD+NEWXy6Y62+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhwan-dj/S4pjt_NLP_koBERT/blob/main/kobert_sentiment_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 개요 : KoBERT를 이용하여 한국어 문장을 여러 클래스로 분류하는 모델을 만드는 것인데, 공포, 놀람, 분노, 슬픔, 중립, 행복, 혐오와 같은 감정이 느껴지는 짧은 대화 텍스트를 각각 어떠한 감정의 텍스트인지 분류하는 모델을 만드는 것이다. 예를 들어 \"앗 깜작이야!\" 라는 문장을 입력하면 '놀람'이라는 클래스로 예측을 하도록 학습시키는 것\n",
        "- 코드는 KoBERT 깃허브에 있는 네이버 영화평 이중분류 예시 코드를 바탕으로 작성\n",
        "- 참고 블로그 (https://velog.io/@seolini43/KOBERT%EB%A1%9C-%EB%8B%A4%EC%A4%91-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-%ED%8C%8C%EC%9D%B4%EC%8D%ACColab)"
      ],
      "metadata": {
        "id": "Oje0O3RuKV25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Colab 환경설정\n",
        "\n",
        "- 라이브러리, 모듈 설치, koBERT 모델 불러오기\n",
        "- 예시 코드의 Dataset = '한국어 감정 정보가 포함된 단발성 대화'\n",
        "- 본 코드의 Dataset : 감성대화말뭉치(https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)\n",
        "\n",
        "<세팅 및 파라미터>  \n",
        "\n",
        "    Python >= 3.6\n",
        "    PyTorch >= 1.70\n",
        "    Transformers = 3.0.2\n",
        "    Colab\n",
        "    batch size = 64\n",
        "    epochs = 10"
      ],
      "metadata": {
        "id": "XyYQb8zYwHAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zFcX0_4ZqycT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c927aec-674f-4918-ff82-9781191122bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.25.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.33)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp38-cp38-linux_x86_64.whl size=689018 sha256=f1ca987926b3ee9574d2b7ec3812df0f61b5e48f0fadbfe0110f89c02cb3fa11\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/93/9d/2237550c409eb3ed725d6302b7897ddd9a037b40cef66dcd9c\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (3.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2.25.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7d902b81636a686a1ea9a50a48439e383a2d04a47d0d1f4cfe4b524f3e244aa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg7U94tC0-k0",
        "outputId": "b68aebc4-213e-4adb-93ac-21ceda34c216"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=4143f462e4918c0b7bc0fb9d37379d5048f02f99eb14ce69b89a155020652b09\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 KoBERT 파일들 다운로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPR0dsBSuvk6",
        "outputId": "a6926b19-4fc3-4be4-83ba-ecc8d7601d60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-x8yds4si\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-x8yds4si\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
            "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece<=0.1.96,>=0.1.6\n",
            "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.10.1,>=1.7.0\n",
            "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers<=4.8.1,>=4.8.1\n",
            "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.19.0,>=1.18.18\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=087e51a052985b53179f24a07ab02f3ddfef5a182a30ca5b7ef231ee572051a6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p0v8k833/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
            "Successfully built kobert\n",
            "Installing collected packages: tokenizers, sentencepiece, torch, onnxruntime, jmespath, mxnet, huggingface-hub, botocore, transformers, s3transfer, boto3, kobert\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: mxnet\n",
            "    Found existing installation: mxnet 1.9.1\n",
            "    Uninstalling mxnet-1.9.1:\n",
            "      Successfully uninstalled mxnet-1.9.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 sentencepiece-0.1.96 tokenizers-0.10.3 torch-1.10.1 transformers-4.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf 의 kobert_tokenizer 폴더를 다운받는 코드.\n",
        "#!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "49l5gc2GV_FK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch             # pytorch framework\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "rwUKuWn_vbLv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert/utils 의 utils.py 파일에서 get_tokenizer 메서드를 불러오는 코드\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert 의 pytorch_kobert.py 파일에서 get_pytorch_kobert_model 메서드를 불러오는 코드\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "LVAfDM7Avd1j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "4JtKK4BTx1nL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "id": "cVlkvld_Okov",
        "outputId": "13e974f5-2493-4831-b51f-39d96080f3a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT는 이미 누군가가 학습해둔 모델을 사용한다(pre-trained model)는 것을 뜻한다. 따라서 사용하는 model과 tokenizer는 항상 mapping 관계여야 한다. 예를 들어서 U 팀이 개발한 BERT를 사용하는데, V팀이 개발한 BERT의 tokenizer를 사용하면 model은 텍스트를 이해할 수 없다. U팀의 BERT의 토크나이저는 '우리'라는 단어를 23번으로 int encoding하는 반면에, V라는 BERT의 tokenizer는 '우리'라는 단어를 103번으로 int encoding해 단어와 mapping 되는 정보 자체가 달라지기 때문이다. \n",
        "(https://hoit1302.tistory.com/159)"
      ],
      "metadata": {
        "id": "Ka3qpO3XY2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터셋 로드, 전처리"
      ],
      "metadata": {
        "id": "wG8D3KQt8MHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1j0eAcBL3_",
        "outputId": "073cde7e-6683-4cb3-b0e1-71a4cde41c8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_excel('/content/drive/MyDrive/data/sentiment_train.xlsx')\n",
        "val = pd.read_excel('/content/drive/MyDrive/data/sentiment_val.xlsx')"
      ],
      "metadata": {
        "id": "iCMNl6xuCyVn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train,val])"
      ],
      "metadata": {
        "id": "QxiSz8zpG_R-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df) # 문장 = 58,271개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B14hRRKkDD4D",
        "outputId": "f8225389-ee0a-4cfb-d771-0c502fd45c47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58271"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "99ttolBtDPyd",
        "outputId": "0ddb5b40-2c3a-4a3d-932d-1b7950dfc3fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0   연령  성별     상황키워드  신체질환 감정_대분류 감정_소분류  \\\n",
              "30183       30184  청소년  여성      가족관계  해당없음     기쁨   신이 난   \n",
              "35587       35588  청소년  여성   학업 및 진로  해당없음     당황    고립된   \n",
              "1794         1795  청소년  남성  학교폭력/따돌림  해당없음     불안    두려운   \n",
              "231           232   청년  남성  연애,결혼,출산  해당없음     분노   방어적인   \n",
              "42060       42061   청년  여성      대인관계  해당없음     분노   방어적인   \n",
              "\n",
              "                                                   사람문장1  \\\n",
              "30183                                        오늘 아빠랑 여행가.   \n",
              "35587  나는 아직도 모르겠는데 학교 친구들은 벌써 미래희망이 있더라. 나 혼자 고립된 느낌이야.   \n",
              "1794                    나 너무 힘들어서 학교 가기 싫어. 친구들 때문에 무서워.   \n",
              "231                              아직 확신이 들지도 않고 결혼이 망설여져.   \n",
              "42060           왜 난 하는 일마다 다 망치는 걸까? 난 정말 쓸모없는 인간인 것 같아.   \n",
              "\n",
              "                                                  시스템문장1  \\\n",
              "30183                 아버지와 여행을 계획하시는군요. 더 자세히 말씀해 주시겠어요?   \n",
              "35587     친구분들과 사정이 달라서 혼자 소외된 느낌을 받으셨군요. 좋아하는 일은 없으신가요?   \n",
              "1794                                  친구들이 어떨 때 가장 무섭나요?   \n",
              "231    결혼을 망설이고 계시는군요. 왜 그렇게 생각을 하시는지 좀 더 자세히 얘기해 주실 ...   \n",
              "42060                        쓸모가 없는 것 같아서 스스로 크게 실망하셨군요.   \n",
              "\n",
              "                                                사람문장2  \\\n",
              "30183                                   바다로 가! 너무 좋아.   \n",
              "35587  동물 키우는걸 좋아하지만 진로는 아니야. 시험공부 하기도 힘겨워서 생각 안 해봤어.   \n",
              "1794          빨리 빵 안 사 온다고 때릴 때 가장 무서워. 난 정말 빨리 간 건데.   \n",
              "231     결혼 자금으로 돈을 충분히 모으지도 못했고 또 결혼 생활이 행복할지 잘 모르겠어.   \n",
              "42060          남들은 알아서 잘사는 것 같은데 나만 아무것도 못 하는 쓰레기 같아.   \n",
              "\n",
              "                               시스템문장2                             사람문장3  \\\n",
              "30183  아버지와 함께 바다에 가신다니 정말 즐거울 것 같아요.                               NaN   \n",
              "35587  시험공부 하느라 진로에 대해 생각할 여유가 없으시군요.                               NaN   \n",
              "1794   빵 사 오라고 괴롭힐 때마다 정말 괴로우실 것 같아요.               그래도 용기 내서 학교는 가야겠지.   \n",
              "231       결혼 후 생활이 행복할지 아직 확신이 없으시군요.                               NaN   \n",
              "42060       그동안 이런 마음이 들 때는 무엇을 하셨나요?  언니에게 하소연하면서 울었어. 언니는 내 말을 잘 들어줘.   \n",
              "\n",
              "                              시스템문장3  \n",
              "30183                            NaN  \n",
              "35587                            NaN  \n",
              "1794        힘들어도 학교에 가겠다는 의지가 있으시군요.  \n",
              "231                              NaN  \n",
              "42060  언니와 대화하면서 속상한 마음이 사라졌으면 좋겠어요.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-431770b1-cd85-4bd6-82e4-e433ebd36200\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>연령</th>\n",
              "      <th>성별</th>\n",
              "      <th>상황키워드</th>\n",
              "      <th>신체질환</th>\n",
              "      <th>감정_대분류</th>\n",
              "      <th>감정_소분류</th>\n",
              "      <th>사람문장1</th>\n",
              "      <th>시스템문장1</th>\n",
              "      <th>사람문장2</th>\n",
              "      <th>시스템문장2</th>\n",
              "      <th>사람문장3</th>\n",
              "      <th>시스템문장3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30183</th>\n",
              "      <td>30184</td>\n",
              "      <td>청소년</td>\n",
              "      <td>여성</td>\n",
              "      <td>가족관계</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>기쁨</td>\n",
              "      <td>신이 난</td>\n",
              "      <td>오늘 아빠랑 여행가.</td>\n",
              "      <td>아버지와 여행을 계획하시는군요. 더 자세히 말씀해 주시겠어요?</td>\n",
              "      <td>바다로 가! 너무 좋아.</td>\n",
              "      <td>아버지와 함께 바다에 가신다니 정말 즐거울 것 같아요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35587</th>\n",
              "      <td>35588</td>\n",
              "      <td>청소년</td>\n",
              "      <td>여성</td>\n",
              "      <td>학업 및 진로</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>당황</td>\n",
              "      <td>고립된</td>\n",
              "      <td>나는 아직도 모르겠는데 학교 친구들은 벌써 미래희망이 있더라. 나 혼자 고립된 느낌이야.</td>\n",
              "      <td>친구분들과 사정이 달라서 혼자 소외된 느낌을 받으셨군요. 좋아하는 일은 없으신가요?</td>\n",
              "      <td>동물 키우는걸 좋아하지만 진로는 아니야. 시험공부 하기도 힘겨워서 생각 안 해봤어.</td>\n",
              "      <td>시험공부 하느라 진로에 대해 생각할 여유가 없으시군요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>1795</td>\n",
              "      <td>청소년</td>\n",
              "      <td>남성</td>\n",
              "      <td>학교폭력/따돌림</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>불안</td>\n",
              "      <td>두려운</td>\n",
              "      <td>나 너무 힘들어서 학교 가기 싫어. 친구들 때문에 무서워.</td>\n",
              "      <td>친구들이 어떨 때 가장 무섭나요?</td>\n",
              "      <td>빨리 빵 안 사 온다고 때릴 때 가장 무서워. 난 정말 빨리 간 건데.</td>\n",
              "      <td>빵 사 오라고 괴롭힐 때마다 정말 괴로우실 것 같아요.</td>\n",
              "      <td>그래도 용기 내서 학교는 가야겠지.</td>\n",
              "      <td>힘들어도 학교에 가겠다는 의지가 있으시군요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>232</td>\n",
              "      <td>청년</td>\n",
              "      <td>남성</td>\n",
              "      <td>연애,결혼,출산</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>분노</td>\n",
              "      <td>방어적인</td>\n",
              "      <td>아직 확신이 들지도 않고 결혼이 망설여져.</td>\n",
              "      <td>결혼을 망설이고 계시는군요. 왜 그렇게 생각을 하시는지 좀 더 자세히 얘기해 주실 ...</td>\n",
              "      <td>결혼 자금으로 돈을 충분히 모으지도 못했고 또 결혼 생활이 행복할지 잘 모르겠어.</td>\n",
              "      <td>결혼 후 생활이 행복할지 아직 확신이 없으시군요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42060</th>\n",
              "      <td>42061</td>\n",
              "      <td>청년</td>\n",
              "      <td>여성</td>\n",
              "      <td>대인관계</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>분노</td>\n",
              "      <td>방어적인</td>\n",
              "      <td>왜 난 하는 일마다 다 망치는 걸까? 난 정말 쓸모없는 인간인 것 같아.</td>\n",
              "      <td>쓸모가 없는 것 같아서 스스로 크게 실망하셨군요.</td>\n",
              "      <td>남들은 알아서 잘사는 것 같은데 나만 아무것도 못 하는 쓰레기 같아.</td>\n",
              "      <td>그동안 이런 마음이 들 때는 무엇을 하셨나요?</td>\n",
              "      <td>언니에게 하소연하면서 울었어. 언니는 내 말을 잘 들어줘.</td>\n",
              "      <td>언니와 대화하면서 속상한 마음이 사라졌으면 좋겠어요.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-431770b1-cd85-4bd6-82e4-e433ebd36200')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-431770b1-cd85-4bd6-82e4-e433ebd36200 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-431770b1-cd85-4bd6-82e4-e433ebd36200');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6번째 칼럼 '감정_대분류'를 라벨 삼아 입력문장의 감정을 예측하기로 한다."
      ],
      "metadata": {
        "id": "LY64-ub8Di0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.감정_대분류.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZ9TM1zDd9R",
        "outputId": "04ffca7a-83dd-4035-cfd3-b1e3737c133b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "불안    10433\n",
              "분노    10417\n",
              "상처    10150\n",
              "슬픔    10128\n",
              "당황     9804\n",
              "기쁨     7339\n",
              "Name: 감정_대분류, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 주관적으로 긍정에서 부정으로 숫자 라벨을 매겨보았다.\n",
        "df.loc[(df['감정_대분류'] == \"기쁨\"), 'sentiment'] = '0'    #기쁨 => 0\n",
        "df.loc[(df['감정_대분류'] == \"당황\"), 'sentiment'] = '1'    #당황 => 1\n",
        "df.loc[(df['감정_대분류'] == \"불안\"), 'sentiment'] = '2'    #불안 => 2\n",
        "df.loc[(df['감정_대분류'] == \"분노\"), 'sentiment'] = '3'    #분노 => 3\n",
        "df.loc[(df['감정_대분류'] == \"슬픔\"), 'sentiment'] = '4'    #슬픔 => 4\n",
        "df.loc[(df['감정_대분류'] == \"상처\"), 'sentiment'] = '5'    #상처 => 5"
      ],
      "metadata": {
        "id": "ty__JODNEHfx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장과 문장의 감정을 리스트로 저장\n",
        "data_list = []\n",
        "for q, label in zip(df['사람문장1'], df['sentiment']):\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)"
      ],
      "metadata": {
        "id": "dUQqLxHKEt2x"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_list[0])\n",
        "print(data_list[6000])\n",
        "print(data_list[12000])\n",
        "print(data_list[18000])\n",
        "print(data_list[24000])\n",
        "print(data_list[30000])\n",
        "print(data_list[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzBNX_QEFdjV",
        "outputId": "58660299-a7ee-481b-d838-c779a7d2788a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['일은 왜 해도 해도 끝이 없을까? 화가 난다.', '3']\n",
            "['최근에 업무가 너무 많이 늘어난 것 같아 힘들어.', '3']\n",
            "['친구들은 다 취업에 성공했는데 나만 못한 것 같아.', '2']\n",
            "['친구에게 내가 간암에 걸려서 술을 마실 수 없다고 하자 거짓말하지 말라고 했어.', '5']\n",
            "['의사가 분명히 수술이 잘 되었다고 했거든. 그런데 삼 주가 되도록 몸을 움직일 수 없어.', '5']\n",
            "['이번에 주식을 샀는데 주가가 너무 떨어져 손해가 이만저만이 아냐.', '4']\n",
            "['친구들 모두 결혼하고 나만 혼자 남아서 쓸쓸하네.', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "Itt1ysump6gB",
        "outputId": "7ef595ae-5293-4756-f667-101f19834346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.6/465.6 KB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import nltk, re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = ['같아', '스러워','하고','만','해서','도','에게','안','못','너무','내','나','에서','해','로','한테','했는데',\n",
        "             '을', '에', '를', '은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한',\n",
        "             '했어', '하는', '게', '들어', '말', '으로','이야', '오늘','요즘','이번','할','봐','돼', '때문', '잘','정말'\n",
        "             ]\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "FFBD5hArsQnB",
        "outputId": "7e3f5430-670b-4724-8eff-f08cebe78e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    # 구두점 제거\n",
        "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
        "\n",
        "    # 소문자로 변환\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # 토큰으로 분리\n",
        "    tokens = okt.morphs(sentence)\n",
        "\n",
        "    # stopwords 제거\n",
        "    tokens = [token for token in tokens if not token in stopwords]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def count_tokens(dataset, label):\n",
        "    # 데이터셋에서 label이 label인 문장만 추출\n",
        "    sentences = [dataset[i][0] for i in range(len(dataset)) if dataset[i][1] == str(label)]\n",
        "    \n",
        "    # 불용어 제거하고 토큰으로 분리\n",
        "    tokens = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "    \n",
        "    # combine all the tokens into a single list\n",
        "    all_tokens = [token for sublist in tokens for token in sublist]\n",
        "    \n",
        "    # 토큰들의 빈도수 측정\n",
        "    counter = Counter(all_tokens)\n",
        "    \n",
        "    return counter\n",
        "\n",
        "# label 0의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 0)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"기쁨 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "Pp7EQPwZp9zj",
        "outputId": "e6b9880b-0084-4c8f-bf91-b8f2e9836cf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기쁨 top_10 :  [('친구', 885), ('기뻐', 708), ('정말', 628), ('잘', 614), ('좋아', 548), ('우리', 497), ('회사', 428), ('다행', 409), ('기분', 405), ('사람', 370)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 1의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 1)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"당황 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "bzliYqBVrY8z",
        "outputId": "20c7e091-c352-41a8-dc6b-0bc79bab873c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "당황 top_10 :  [('친구', 1872), ('당황', 928), ('사람', 710), ('회사', 472), ('아내', 446), ('돈', 433), ('부모님', 432), ('일', 427), ('죄책감', 424), ('남편', 410)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 1의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 2)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"불안 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "0SubD0w5xmEM",
        "outputId": "02085ac9-d8a0-4e2b-ad21-82a0947c0e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불안 top_10 :  [('친구', 1316), ('걱정', 1040), ('불안해', 883), ('사람', 520), ('회사', 514), ('일', 512), ('해야', 496), ('돈', 482), ('스트레스', 463), ('하는데', 422)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 3)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"분노 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "BfNifxZ7x1v8",
        "outputId": "879107ad-9b7d-4ceb-a1a0-f7220f655269",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분노 top_10 :  [('친구', 1524), ('화가', 1282), ('짜증', 627), ('남편', 614), ('사람', 598), ('돈', 520), ('아내', 517), ('일', 514), ('자꾸', 504), ('고', 465)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 4)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"슬픔 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "0qN4Rtyox7zq",
        "outputId": "25767f47-47d9-4ff9-8d06-4b1a6f0438f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슬픔 top_10 :  [('슬퍼', 1406), ('친구', 1405), ('생각', 524), ('사람', 497), ('일', 486), ('회사', 474), ('돈', 467), ('아내', 464), ('부모님', 458), ('눈물', 446)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 5)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"상처 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "WYaTwRgGx9hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train과 Test Set 준비"
      ],
      "metadata": {
        "id": "5xwTeWE5GMNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.15, random_state=0)"
      ],
      "metadata": {
        "id": "NKN0ZNIRGJFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "id": "S4lvU3MsH8Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. koBERT의 입력 데이터 형태로 만들기\n",
        "- 모델의 입력으로 활용하기 위해 토큰화, 라벨과 문장의 인덱스 인코딩, 패딩 등이 필요하다.\n",
        "- 예시 코드의 입력데이터 형태로 바꿔주는 클래스 활용"
      ],
      "metadata": {
        "id": "XmoD074KJ3c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "      transform = nlp.data.BERTSentenceTransform(\n",
        "          bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)   # BERTSentenceTransform 으로 토큰화, 패딩\n",
        "      \n",
        "      self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "      #self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "      self.labels = [str(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "      return (len(self.labels))"
      ],
      "metadata": {
        "id": "-K1U1yuEH_FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "hqmFQWIjM-Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTTokenizer와 위의 Class로 Tokenize, Padding\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "metadata": {
        "id": "fVNyGYpXNDUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화와 패딩이 잘 이루어졌을까?\n",
        "- 데이터는 3개의 array인데, 첫번째는 패딩된 시퀀스, 두번째는 길이와 타입, 세번째는 어텐션 마스크 시퀀스라고 한다.\n",
        "- BERT에 데이터가 입력되었을 때 어텐션 함수가 적용되어 연산이 된다. 이때 1로 패딩된 값들은 연산할 필요가 없기 때문에 연산을 하지 않아도 된다고 알려주는 데이터가 있어야 하는데 그게 바로 어텐션 마스크 시퀀스 라고 한다. (아직 이해x)"
      ],
      "metadata": {
        "id": "E6fW6dhtPVh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0]"
      ],
      "metadata": {
        "id": "dY1QXyoWPIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch 형식의 dataset을 만들어준다.\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "_VZ9R-xLRpQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. koBERT 학습모델 만들기\n",
        "- num_classes = 6 (6개의 감정 클래스)"
      ],
      "metadata": {
        "id": "oFu0VLwFR00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 6,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "btj-LLsARz8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정(linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy() / max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "EvPDiE8YSHYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. BERT model 학습시키기"
      ],
      "metadata": {
        "id": "B-sS6lmYSebv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "xmh8YV1eSdHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Epochs 5 학습은 약 47분이 걸렸고, train dataset에 대해서는 0.747, test dataset에 대해서는 0.594의 정확도를 기록했다.\n",
        "\n",
        "- 이거 좀 낮은데;;\n",
        "\n",
        "- 참고블로그에서는 정확도가 높았는데, 이에 대한 설명 :\n",
        "train dataset에 대해서는 0.979, test dataset에 대해서는 0.918의 정확도를 기록\n",
        "이렇게 높은 정확도를 기록하는 이유는 바로 데이터셋에 있다. 진행하고 있는 프로젝트에서 해당 프로젝트의 성격에 맞게 3만 7천 여개 정도의 데이터를 직접 재분류하고 있는데, 사람이 생각하기에는 분명히 같은 내용으로 인지되는 문장이지만 사소한 단어를 한 두개 없애서, 어떨 때는 중요한 단어를 제거해서, 짧게 잘라서, 길게 늘여뜨려서, 문장 부호를 다르게, 감탄사를 추가해서 등등 중복인 듯 중복 아닌 데이터로 학습시켰기 때문에 이렇게 높은 정확도가 나온 것이라 판단된다.\n",
        "(https://hoit1302.tistory.com/159)"
      ],
      "metadata": {
        "id": "bENXuiMwZthQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "\n",
        "class F1Score:\n",
        "    \"\"\"\n",
        "    Class for f1 calculation in Pytorch.\n",
        "    \"\"\"\n",
        "    def __init__(self, average: str = 'weighted'):\n",
        "        \"\"\"\n",
        "        Init.\n",
        "\n",
        "        Args:\n",
        "            average: averaging method\n",
        "        \"\"\"\n",
        "        self.average = average\n",
        "        if average not in [None, 'micro', 'macro', 'weighted']:\n",
        "            raise ValueError('Wrong value of average parameter')\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 micro.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "        true_positive = torch.eq(labels, predictions).sum().float()\n",
        "        f1_score = torch.div(true_positive, len(labels))\n",
        "        return f1_score\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_f1_count_for_label(predictions: torch.Tensor,\n",
        "                                labels: torch.Tensor, label_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calculate f1 and true count for the label\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "            label_id: id of current label\n",
        "\n",
        "        Returns:\n",
        "            f1 score and true count for label\n",
        "        \"\"\"\n",
        "        # label count\n",
        "        true_count = torch.eq(labels, label_id).sum()\n",
        "\n",
        "        # true positives: labels equal to prediction and to label_id\n",
        "        true_positive = torch.logical_and(torch.eq(labels, predictions),\n",
        "                                          torch.eq(labels, label_id)).sum().float()\n",
        "        # precision for label\n",
        "        precision = torch.div(true_positive, torch.eq(predictions, label_id).sum().float())\n",
        "        # replace nan values with 0\n",
        "        precision = torch.where(torch.isnan(precision),\n",
        "                                torch.zeros_like(precision).type_as(true_positive),\n",
        "                                precision)\n",
        "\n",
        "        # recall for label\n",
        "        recall = torch.div(true_positive, true_count)\n",
        "        # f1\n",
        "        f1 = 2 * precision * recall / (precision + recall)\n",
        "        # replace nan values with 0\n",
        "        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1).type_as(true_positive), f1)\n",
        "        return f1, true_count\n",
        "\n",
        "    def __call__(self, predictions: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate f1 score based on averaging method defined in init.\n",
        "\n",
        "        Args:\n",
        "            predictions: tensor with predictions\n",
        "            labels: tensor with original labels\n",
        "\n",
        "        Returns:\n",
        "            f1 score\n",
        "        \"\"\"\n",
        "\n",
        "        # simpler calculation for micro\n",
        "        if self.average == 'micro':\n",
        "            return self.calc_f1_micro(predictions, labels)\n",
        "\n",
        "        f1_score = 0\n",
        "        for label_id in range(1, len(labels.unique()) + 1):\n",
        "            f1, true_count = self.calc_f1_count_for_label(predictions, labels, label_id)\n",
        "\n",
        "            if self.average == 'weighted':\n",
        "                f1_score += f1 * true_count\n",
        "            elif self.average == 'macro':\n",
        "                f1_score += f1\n",
        "\n",
        "        if self.average == 'weighted':\n",
        "            f1_score = torch.div(f1_score, len(labels))\n",
        "        elif self.average == 'macro':\n",
        "            f1_score = torch.div(f1_score, len(labels.unique()))\n",
        "\n",
        "        return f1_score"
      ],
      "metadata": {
        "id": "L5D1x3iO5Ev0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "errors = 0\n",
        "for _ in range(10):\n",
        "    labels = torch.randint(1, 10, (4096, 100)).flatten()\n",
        "    predictions = torch.randint(1, 10, (4096, 100)).flatten()\n",
        "    labels1 = labels.numpy()\n",
        "    predictions1 = predictions.numpy()\n",
        "\n",
        "    for av in ['micro', 'macro', 'weighted']:\n",
        "        f1_metric = F1Score(av)\n",
        "        my_pred = f1_metric(predictions, labels)\n",
        "        \n",
        "        f1_pred = f1_score(labels1, predictions1, average=av)\n",
        "        \n",
        "        if not np.isclose(my_pred.item(), f1_pred.item()):\n",
        "            print('!' * 50)\n",
        "            print(f1_pred, my_pred, av)\n",
        "            errors += 1\n",
        "\n",
        "if errors == 0:\n",
        "    print('No errors!')"
      ],
      "metadata": {
        "id": "U19EE3HX5Jxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 새로운 문장 테스트\n",
        "- 새로운 문장을 koBERT의 입력 형식으로 바꿔주는 predict 함수를 정의한다."
      ],
      "metadata": {
        "id": "GiDhEI8hTM_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"기쁨이\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"당황이\")\n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"불안이\")\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"분노가\")\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"슬픔이\")\n",
        "            elif np.argmax(logits) == 5:\n",
        "                test_eval.append(\"상처가\")\n",
        "\n",
        "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")"
      ],
      "metadata": {
        "id": "9EjdYGbRTiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문 무한반복하기! 0 입력시 종료\n",
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if sentence == '0' :\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "p-tyA1rraRq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Result**\n",
        "- 학습데이터 정확도\n",
        "- 테스트데이터 정확도\n",
        "- F1, Precision, Recall 점수를 계산\n",
        "- 새로운 문장에 대한 정확도\n",
        "\n",
        "- 한계점\n",
        "\n",
        "- 개선점\n",
        "\n",
        "- 기대효과"
      ],
      "metadata": {
        "id": "TediliL4a6Ka"
      }
    }
  ]
}