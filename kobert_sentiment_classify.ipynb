{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4bob1La/sp8LnnVqdmCZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhwan-dj/S4pjt_NLP_koBERT/blob/main/kobert_sentiment_classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 개요 : KoBERT를 이용하여 한국어 문장을 여러 클래스로 분류하는 모델을 만드는 것인데, 공포, 놀람, 분노, 슬픔, 중립, 행복, 혐오와 같은 감정이 느껴지는 짧은 대화 텍스트를 각각 어떠한 감정의 텍스트인지 분류하는 모델을 만드는 것이다. 예를 들어 \"앗 깜작이야!\" 라는 문장을 입력하면 '놀람'이라는 클래스로 예측을 하도록 학습시키는 것\n",
        "- 코드는 KoBERT 깃허브에 있는 네이버 영화평 이중분류 예시 코드를 바탕으로 작성\n",
        "- 참고 블로그 (https://velog.io/@seolini43/KOBERT%EB%A1%9C-%EB%8B%A4%EC%A4%91-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-%ED%8C%8C%EC%9D%B4%EC%8D%ACColab)"
      ],
      "metadata": {
        "id": "Oje0O3RuKV25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Colab 환경설정\n",
        "\n",
        "- 라이브러리, 모듈 설치, koBERT 모델 불러오기\n",
        "- 예시 코드의 Dataset = '한국어 감정 정보가 포함된 단발성 대화'\n",
        "- 본 코드의 Dataset : 감성대화말뭉치(https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=86)\n",
        "\n",
        "<세팅 및 파라미터>  \n",
        "\n",
        "    Python >= 3.6\n",
        "    PyTorch >= 1.70\n",
        "    Transformers = 3.0.2\n",
        "    Colab\n",
        "    batch size = 64\n",
        "    epochs = 10"
      ],
      "metadata": {
        "id": "XyYQb8zYwHAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "zFcX0_4ZqycT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4195e9af-ded0-410a-ba84-97e2bc1f1a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.8/dist-packages (1.7.0.post2)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (2.25.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.8/dist-packages (from mxnet) (1.21.6)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (0.29.33)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp) (23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.0.2\n",
            "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Using cached tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (2022.6.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (0.0.53)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==3.0.2) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.10.3\n",
            "    Uninstalling tokenizers-0.10.3:\n",
            "      Successfully uninstalled tokenizers-0.10.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.8.1\n",
            "    Uninstalling transformers-4.8.1:\n",
            "      Successfully uninstalled transformers-4.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kobert 0.2.3 requires transformers<=4.8.1,>=4.8.1, but you have transformers 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg7U94tC0-k0",
        "outputId": "2b70a8e6-cf24-4556-dd75-e53b81f73aff"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 KoBERT 파일들 다운로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MPR0dsBSuvk6",
        "outputId": "b8b3f71d-450c-4d57-d4bf-3f967c38890c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-7_mezdoe\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-7_mezdoe\n",
            "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: boto3<=1.15.18 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.15.18)\n",
            "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.7.0.post2)\n",
            "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.8.0)\n",
            "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (0.1.96)\n",
            "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from kobert==0.2.3) (1.10.1)\n",
            "Collecting transformers<=4.8.1,>=4.8.1\n",
            "  Using cached transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.6)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.6)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (23.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.33)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.9.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.53)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20 in /usr/local/lib/python3.8/dist-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.8.1rc1\n",
            "    Uninstalling tokenizers-0.8.1rc1:\n",
            "      Successfully uninstalled tokenizers-0.8.1rc1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.0.2\n",
            "    Uninstalling transformers-3.0.2:\n",
            "      Successfully uninstalled transformers-3.0.2\n",
            "Successfully installed tokenizers-0.10.3 transformers-4.8.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert_hf 의 kobert_tokenizer 폴더를 다운받는 코드.\n",
        "#!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "49l5gc2GV_FK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch             # pytorch framework\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "rwUKuWn_vbLv"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert/utils 의 utils.py 파일에서 get_tokenizer 메서드를 불러오는 코드\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "# https://github.com/SKTBrain/KoBERT/tree/master/kobert 의 pytorch_kobert.py 파일에서 get_pytorch_kobert_model 메서드를 불러오는 코드\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "LVAfDM7Avd1j"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "4JtKK4BTx1nL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "id": "cVlkvld_Okov",
        "outputId": "3953d8b0-7d22-42f9-f3ca-2a60fd3689a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_v1.zip\n",
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT는 이미 누군가가 학습해둔 모델을 사용한다(pre-trained model)는 것을 뜻한다. 따라서 사용하는 model과 tokenizer는 항상 mapping 관계여야 한다. 예를 들어서 U 팀이 개발한 BERT를 사용하는데, V팀이 개발한 BERT의 tokenizer를 사용하면 model은 텍스트를 이해할 수 없다. U팀의 BERT의 토크나이저는 '우리'라는 단어를 23번으로 int encoding하는 반면에, V라는 BERT의 tokenizer는 '우리'라는 단어를 103번으로 int encoding해 단어와 mapping 되는 정보 자체가 달라지기 때문이다. \n",
        "(https://hoit1302.tistory.com/159)"
      ],
      "metadata": {
        "id": "Ka3qpO3XY2-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터셋 로드, 전처리"
      ],
      "metadata": {
        "id": "wG8D3KQt8MHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#구글드라이브 연동\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ1j0eAcBL3_",
        "outputId": "096dc3cc-3b99-4b2f-8797-963310dff0a5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_excel('/content/drive/MyDrive/data/sentiment_train.xlsx')\n",
        "val = pd.read_excel('/content/drive/MyDrive/data/sentiment_val.xlsx')"
      ],
      "metadata": {
        "id": "iCMNl6xuCyVn"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train,val])"
      ],
      "metadata": {
        "id": "QxiSz8zpG_R-"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df) # 문장 = 58,271개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B14hRRKkDD4D",
        "outputId": "fd379164-c3b6-4c28-8675-364e5cf96503"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58271"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "99ttolBtDPyd",
        "outputId": "728ea277-f53a-4d3b-dd2d-81a29fab3a46"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Unnamed: 0   연령  성별     상황키워드    신체질환 감정_대분류 감정_소분류  \\\n",
              "4311         4312   청년  남성      대인관계    해당없음     불안    초조한   \n",
              "41346       41347   노년  여성      대인관계  만성질환 무     슬픔    마비된   \n",
              "30118       30119  청소년  남성  학교폭력/따돌림    해당없음     분노     분노   \n",
              "33278       33279   청년  남성      대인관계    해당없음     당황    외로운   \n",
              "29235       29236   노년  여성     건강,죽음  만성질환 무     불안    초조한   \n",
              "\n",
              "                                사람문장1                                시스템문장1  \\\n",
              "4311          친구들이 이유 없이 날 싫어하는 것 같아.                       왜 그렇게 생각하게 되셨죠?   \n",
              "41346                 만나서 대화할 친구가 없어.            말동무가 없어서 무척이나 적적하신 모양입니다.    \n",
              "30118          학교에서 따돌림 당해서 화를 못 참겠어.  교내 따돌림을 당해서 기분이 안좋으시군요. 어떻게 하면 좋을까요?   \n",
              "33278  내 주변에는 친구가 별로 없는 것 같아서 많이 외로워.               친구가 없어 외로움을 크게 느끼나 보네요.   \n",
              "29235                       친구가 아프다네.   친구가 아프셔서 걱정이 많으시겠어요. 앞으로 어떻게 하실건가요?   \n",
              "\n",
              "                                                   사람문장2  \\\n",
              "4311           오늘도 같이 매점에 나만 빼고 가더라고. 이렇게 계속 멀어질까 봐 초조해.   \n",
              "41346  매일 집에만 있으니 우울하고 외로운데 주변에서 마음 맞는 사람을 찾기도 참 힘이 드네.    \n",
              "30118                           친구들과 대화를 시도해서 해결해 보려고 해.   \n",
              "33278                     이젠 바깥에도 자주 나가고 대외활동도 많이 해볼까 해.   \n",
              "29235                  나도 아프지 않기 위해 좀더 건강관리에 힘써야 할 것 같아.   \n",
              "\n",
              "                                                  시스템문장2  \\\n",
              "4311        친구와 멀어지고 싶지 않군요. 혹시 먼저 다가가서 얘기한다면 어떨 것 같으세요?   \n",
              "41346  주변에 대화가 편한 상대를 찾기가 어려우신 거군요. 종교생활이나 취미활동을 하고 계...   \n",
              "30118                     학교에서 따돌림을 친구들과 대화로 해결하실 생각이군요.   \n",
              "33278                    외로움을 극복하기 위해 외부 활동을 늘리실 생각이시군요.   \n",
              "29235                     친구분처럼 아프지 않게 건강관리 잘 하실 계획이시군요.   \n",
              "\n",
              "                                  사람문장3  \\\n",
              "4311                                NaN   \n",
              "41346  이제는 사람 만나는 일도 힘에 겨워서 엄두도 못내고 있지.   \n",
              "30118                               NaN   \n",
              "33278                               NaN   \n",
              "29235                               NaN   \n",
              "\n",
              "                                        시스템문장3  \n",
              "4311                                       NaN  \n",
              "41346  사람을 만나고 싶어도 힘에 부치다고 느끼시니 무척 혼란스러우시겠어요.   \n",
              "30118                                      NaN  \n",
              "33278                                      NaN  \n",
              "29235                                      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a3f071a-f4b2-4a36-8adc-3a0d352b3cc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>연령</th>\n",
              "      <th>성별</th>\n",
              "      <th>상황키워드</th>\n",
              "      <th>신체질환</th>\n",
              "      <th>감정_대분류</th>\n",
              "      <th>감정_소분류</th>\n",
              "      <th>사람문장1</th>\n",
              "      <th>시스템문장1</th>\n",
              "      <th>사람문장2</th>\n",
              "      <th>시스템문장2</th>\n",
              "      <th>사람문장3</th>\n",
              "      <th>시스템문장3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4311</th>\n",
              "      <td>4312</td>\n",
              "      <td>청년</td>\n",
              "      <td>남성</td>\n",
              "      <td>대인관계</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>불안</td>\n",
              "      <td>초조한</td>\n",
              "      <td>친구들이 이유 없이 날 싫어하는 것 같아.</td>\n",
              "      <td>왜 그렇게 생각하게 되셨죠?</td>\n",
              "      <td>오늘도 같이 매점에 나만 빼고 가더라고. 이렇게 계속 멀어질까 봐 초조해.</td>\n",
              "      <td>친구와 멀어지고 싶지 않군요. 혹시 먼저 다가가서 얘기한다면 어떨 것 같으세요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41346</th>\n",
              "      <td>41347</td>\n",
              "      <td>노년</td>\n",
              "      <td>여성</td>\n",
              "      <td>대인관계</td>\n",
              "      <td>만성질환 무</td>\n",
              "      <td>슬픔</td>\n",
              "      <td>마비된</td>\n",
              "      <td>만나서 대화할 친구가 없어.</td>\n",
              "      <td>말동무가 없어서 무척이나 적적하신 모양입니다.</td>\n",
              "      <td>매일 집에만 있으니 우울하고 외로운데 주변에서 마음 맞는 사람을 찾기도 참 힘이 드네.</td>\n",
              "      <td>주변에 대화가 편한 상대를 찾기가 어려우신 거군요. 종교생활이나 취미활동을 하고 계...</td>\n",
              "      <td>이제는 사람 만나는 일도 힘에 겨워서 엄두도 못내고 있지.</td>\n",
              "      <td>사람을 만나고 싶어도 힘에 부치다고 느끼시니 무척 혼란스러우시겠어요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30118</th>\n",
              "      <td>30119</td>\n",
              "      <td>청소년</td>\n",
              "      <td>남성</td>\n",
              "      <td>학교폭력/따돌림</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>분노</td>\n",
              "      <td>분노</td>\n",
              "      <td>학교에서 따돌림 당해서 화를 못 참겠어.</td>\n",
              "      <td>교내 따돌림을 당해서 기분이 안좋으시군요. 어떻게 하면 좋을까요?</td>\n",
              "      <td>친구들과 대화를 시도해서 해결해 보려고 해.</td>\n",
              "      <td>학교에서 따돌림을 친구들과 대화로 해결하실 생각이군요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33278</th>\n",
              "      <td>33279</td>\n",
              "      <td>청년</td>\n",
              "      <td>남성</td>\n",
              "      <td>대인관계</td>\n",
              "      <td>해당없음</td>\n",
              "      <td>당황</td>\n",
              "      <td>외로운</td>\n",
              "      <td>내 주변에는 친구가 별로 없는 것 같아서 많이 외로워.</td>\n",
              "      <td>친구가 없어 외로움을 크게 느끼나 보네요.</td>\n",
              "      <td>이젠 바깥에도 자주 나가고 대외활동도 많이 해볼까 해.</td>\n",
              "      <td>외로움을 극복하기 위해 외부 활동을 늘리실 생각이시군요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29235</th>\n",
              "      <td>29236</td>\n",
              "      <td>노년</td>\n",
              "      <td>여성</td>\n",
              "      <td>건강,죽음</td>\n",
              "      <td>만성질환 무</td>\n",
              "      <td>불안</td>\n",
              "      <td>초조한</td>\n",
              "      <td>친구가 아프다네.</td>\n",
              "      <td>친구가 아프셔서 걱정이 많으시겠어요. 앞으로 어떻게 하실건가요?</td>\n",
              "      <td>나도 아프지 않기 위해 좀더 건강관리에 힘써야 할 것 같아.</td>\n",
              "      <td>친구분처럼 아프지 않게 건강관리 잘 하실 계획이시군요.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a3f071a-f4b2-4a36-8adc-3a0d352b3cc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a3f071a-f4b2-4a36-8adc-3a0d352b3cc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a3f071a-f4b2-4a36-8adc-3a0d352b3cc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 6번째 칼럼 '감정_대분류'를 라벨 삼아 입력문장의 감정을 예측하기로 한다."
      ],
      "metadata": {
        "id": "LY64-ub8Di0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.감정_대분류.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZ9TM1zDd9R",
        "outputId": "c7a0bad7-f8bc-4d34-bb27-f714fd35a2de"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "불안    10433\n",
              "분노    10417\n",
              "상처    10150\n",
              "슬픔    10128\n",
              "당황     9804\n",
              "기쁨     7339\n",
              "Name: 감정_대분류, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 주관적으로 긍정에서 부정으로 숫자 라벨을 매겨보았다.\n",
        "df.loc[(df['감정_대분류'] == \"기쁨\"), 'sentiment'] = '0'    #기쁨 => 0\n",
        "df.loc[(df['감정_대분류'] == \"당황\"), 'sentiment'] = '1'    #당황 => 1\n",
        "df.loc[(df['감정_대분류'] == \"불안\"), 'sentiment'] = '2'    #불안 => 2\n",
        "df.loc[(df['감정_대분류'] == \"분노\"), 'sentiment'] = '3'    #분노 => 3\n",
        "df.loc[(df['감정_대분류'] == \"슬픔\"), 'sentiment'] = '4'    #슬픔 => 4\n",
        "df.loc[(df['감정_대분류'] == \"상처\"), 'sentiment'] = '5'    #상처 => 5"
      ],
      "metadata": {
        "id": "ty__JODNEHfx"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장과 문장의 감정을 리스트로 저장\n",
        "data_list = []\n",
        "for q, label in zip(df['사람문장1'], df['sentiment']):\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)"
      ],
      "metadata": {
        "id": "dUQqLxHKEt2x"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_list[0])\n",
        "print(data_list[6000])\n",
        "print(data_list[12000])\n",
        "print(data_list[18000])\n",
        "print(data_list[24000])\n",
        "print(data_list[30000])\n",
        "print(data_list[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzBNX_QEFdjV",
        "outputId": "a21420cf-4dd5-4f25-fd1d-a8a0ef93dddb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['일은 왜 해도 해도 끝이 없을까? 화가 난다.', '3']\n",
            "['최근에 업무가 너무 많이 늘어난 것 같아 힘들어.', '3']\n",
            "['친구들은 다 취업에 성공했는데 나만 못한 것 같아.', '2']\n",
            "['친구에게 내가 간암에 걸려서 술을 마실 수 없다고 하자 거짓말하지 말라고 했어.', '5']\n",
            "['의사가 분명히 수술이 잘 되었다고 했거든. 그런데 삼 주가 되도록 몸을 움직일 수 없어.', '5']\n",
            "['이번에 주식을 샀는데 주가가 너무 떨어져 손해가 이만저만이 아냐.', '4']\n",
            "['친구들 모두 결혼하고 나만 혼자 남아서 쓸쓸하네.', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "Itt1ysump6gB",
        "outputId": "992a59f5-adc2-4f03-d199-fd86b4b69464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "import nltk, re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = ['같아', '스러워','하고','만','해서','도','에게','안','못','너무','내','나','에서','해','로','한테','했는데',\n",
        "             '을', '에', '를', '은', '는', '이', '가', '하', '아', '것', '들', '의', '있', '되', '수', '보', '주', '등', '한',\n",
        "             '했어', '하는', '게', '들어', '말', '으로','이야', '오늘','요즘','이번','할','봐','돼', '때문', '잘','정말','고'\n",
        "             ]\n",
        "okt = Okt()"
      ],
      "metadata": {
        "id": "FFBD5hArsQnB",
        "outputId": "48d0d504-053b-47aa-8d26-35ba5a345079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    # 구두점 제거\n",
        "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
        "\n",
        "    # 소문자로 변환\n",
        "    sentence = sentence.lower()\n",
        "\n",
        "    # 토큰으로 분리\n",
        "    tokens = okt.morphs(sentence)\n",
        "\n",
        "    # stopwords 제거\n",
        "    tokens = [token for token in tokens if not token in stopwords]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def count_tokens(dataset, label):\n",
        "    # 데이터셋에서 label이 label인 문장만 추출\n",
        "    sentences = [dataset[i][0] for i in range(len(dataset)) if dataset[i][1] == str(label)]\n",
        "    \n",
        "    # 불용어 제거하고 토큰으로 분리\n",
        "    tokens = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "    \n",
        "    # combine all the tokens into a single list\n",
        "    all_tokens = [token for sublist in tokens for token in sublist]\n",
        "    \n",
        "    # 토큰들의 빈도수 측정\n",
        "    counter = Counter(all_tokens)\n",
        "    \n",
        "    return counter\n",
        "\n",
        "# label 0의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 0)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"기쁨 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "Pp7EQPwZp9zj",
        "outputId": "b46b7e4a-68cf-45dc-88ef-ea6756303ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "기쁨 top_10 :  [('친구', 885), ('기뻐', 708), ('좋아', 548), ('우리', 497), ('회사', 428), ('다행', 409), ('기분', 405), ('사람', 370), ('있어', 337), ('마음', 334)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 1의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 1)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"당황 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "bzliYqBVrY8z",
        "outputId": "97dbb53a-00c3-42fe-984b-f9e0f783fe73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "당황 top_10 :  [('친구', 1872), ('당황', 928), ('사람', 710), ('회사', 472), ('아내', 446), ('돈', 433), ('부모님', 432), ('일', 427), ('죄책감', 424), ('남편', 410)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# label 1의 top 10 tokens (기쁨 => 0, 당황 => 1, 불안 => 2, 분노 => 3, 슬픔 => 4, 상처 => 5)\n",
        "counter = count_tokens(data_list, 2)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"불안 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "0SubD0w5xmEM",
        "outputId": "53d8d5b5-b1df-4919-a6f1-657840c5bc02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불안 top_10 :  [('친구', 1316), ('걱정', 1040), ('불안해', 883), ('사람', 520), ('회사', 514), ('일', 512), ('해야', 496), ('돈', 482), ('스트레스', 463), ('하는데', 422)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 3)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"분노 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "BfNifxZ7x1v8",
        "outputId": "b299b852-6fd6-4518-c0b4-f9a0ff9db543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분노 top_10 :  [('친구', 1524), ('화가', 1282), ('짜증', 627), ('남편', 614), ('사람', 598), ('돈', 520), ('아내', 517), ('일', 514), ('자꾸', 504), ('고', 465)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 4)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"슬픔 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "0qN4Rtyox7zq",
        "outputId": "d348c0e7-8bca-4d72-df92-12ba97f99db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슬픔 top_10 :  [('슬퍼', 1406), ('친구', 1405), ('생각', 524), ('사람', 497), ('일', 486), ('회사', 474), ('돈', 467), ('아내', 464), ('부모님', 458), ('눈물', 446)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = count_tokens(data_list, 5)\n",
        "top_10 = counter.most_common(10)\n",
        "print(\"상처 top_10 : \", top_10)"
      ],
      "metadata": {
        "id": "WYaTwRgGx9hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4d7636-6484-4310-a73e-508c3f13ab8d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "상처 top_10 :  [('친구', 1655), ('사람', 625), ('돈', 599), ('남편', 583), ('나를', 570), ('회사', 519), ('일', 506), ('적', 489), ('힘들어', 418), ('우리', 414)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train과 Test Set 준비"
      ],
      "metadata": {
        "id": "5xwTeWE5GMNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.15, random_state=0)"
      ],
      "metadata": {
        "id": "NKN0ZNIRGJFw"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "id": "S4lvU3MsH8Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd38045-8df3-4b8b-e73a-46e8576157c9"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49530\n",
            "8741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. koBERT의 입력 데이터 형태로 만들기\n",
        "- 모델의 입력으로 활용하기 위해 토큰화, 라벨과 문장의 인덱스 인코딩, 패딩 등이 필요하다.\n",
        "- 예시 코드의 입력데이터 형태로 바꿔주는 클래스 활용"
      ],
      "metadata": {
        "id": "XmoD074KJ3c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
        "      transform = nlp.data.BERTSentenceTransform(\n",
        "          bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)   # BERTSentenceTransform 으로 토큰화, 패딩\n",
        "      \n",
        "      self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "      #self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "      self.labels = [str(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "      return (len(self.labels))"
      ],
      "metadata": {
        "id": "-K1U1yuEH_FJ"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "hqmFQWIjM-Hh"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTTokenizer와 위의 Class로 Tokenize, Padding\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "metadata": {
        "id": "fVNyGYpXNDUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d52a6d-7b84-44a8-d427-44668592b2f0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화와 패딩이 잘 이루어졌을까?\n",
        "- 데이터는 3개의 array인데, 첫번째는 패딩된 시퀀스, 두번째는 길이와 타입, 세번째는 어텐션 마스크 시퀀스라고 한다.\n",
        "- BERT에 데이터가 입력되었을 때 어텐션 함수가 적용되어 연산이 된다. 이때 1로 패딩된 값들은 연산할 필요가 없기 때문에 연산을 하지 않아도 된다고 알려주는 데이터가 있어야 하는데 그게 바로 어텐션 마스크 시퀀스 라고 한다. (아직 이해x)"
      ],
      "metadata": {
        "id": "E6fW6dhtPVh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0]"
      ],
      "metadata": {
        "id": "dY1QXyoWPIVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c2e307-f4cb-4fe5-ea99-a2201990aa62"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   2, 1261, 7318, 6941, 7096, 3869,  784, 5579,  889, 6122, 6198,\n",
              "        4688, 7126, 7120, 4304, 4213, 6896, 1934, 4627, 6897,  784, 5573,\n",
              "         517, 6976, 6797, 6323, 6072, 6855,  517,   54,    3,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
              "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
              " array(31, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       dtype=int32),\n",
              " '3')"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch 형식의 dataset을 만들어준다.\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "_VZ9R-xLRpQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c07ab4d-20f9-47a6-b912-7cd2819684c2"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. koBERT 학습모델 만들기\n",
        "- num_classes = 6 (6개의 감정 클래스)"
      ],
      "metadata": {
        "id": "oFu0VLwFR00V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 6,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "btj-LLsARz8h"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정(linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy() / max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "EvPDiE8YSHYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. BERT model 학습시키기"
      ],
      "metadata": {
        "id": "B-sS6lmYSebv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "xmh8YV1eSdHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Epochs 5 학습은 약 47분이 걸렸고, train dataset에 대해서는 0.747, test dataset에 대해서는 0.594의 정확도를 기록했다.\n",
        "\n",
        "- 이거 좀 낮은데;;\n",
        "\n",
        "- 참고블로그에서는 정확도가 높았는데, 이에 대한 설명 :\n",
        "train dataset에 대해서는 0.979, test dataset에 대해서는 0.918의 정확도를 기록\n",
        "이렇게 높은 정확도를 기록하는 이유는 바로 데이터셋에 있다. 진행하고 있는 프로젝트에서 해당 프로젝트의 성격에 맞게 3만 7천 여개 정도의 데이터를 직접 재분류하고 있는데, 사람이 생각하기에는 분명히 같은 내용으로 인지되는 문장이지만 사소한 단어를 한 두개 없애서, 어떨 때는 중요한 단어를 제거해서, 짧게 잘라서, 길게 늘여뜨려서, 문장 부호를 다르게, 감탄사를 추가해서 등등 중복인 듯 중복 아닌 데이터로 학습시켰기 때문에 이렇게 높은 정확도가 나온 것이라 판단된다.\n",
        "(https://hoit1302.tistory.com/159)"
      ],
      "metadata": {
        "id": "bENXuiMwZthQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 새로운 문장 테스트\n",
        "- 새로운 문장을 koBERT의 입력 형식으로 바꿔주는 predict 함수를 정의한다."
      ],
      "metadata": {
        "id": "GiDhEI8hTM_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"기쁨이\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"당황이\")\n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"불안이\")\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"분노가\")\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"슬픔이\")\n",
        "            elif np.argmax(logits) == 5:\n",
        "                test_eval.append(\"상처가\")\n",
        "\n",
        "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")"
      ],
      "metadata": {
        "id": "9EjdYGbRTiJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문 무한반복하기! 0 입력시 종료\n",
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if sentence == '0' :\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "p-tyA1rraRq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Result**\n",
        "- 학습데이터 정확도\n",
        "- 테스트데이터 정확도\n",
        "- F1, Precision, Recall 점수를 계산\n",
        "- 새로운 문장에 대한 정확도\n",
        "\n",
        "- 한계점\n",
        "\n",
        "- 개선점\n",
        "\n",
        "- 기대효과"
      ],
      "metadata": {
        "id": "TediliL4a6Ka"
      }
    }
  ]
}